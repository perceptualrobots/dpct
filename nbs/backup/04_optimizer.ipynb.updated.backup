{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8da121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from dpct.individual import DHPCTIndividual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7575dc",
   "metadata": {},
   "source": [
    "# DHPCTOptimizer\n",
    "\n",
    "> Class for hyperparameter optimization of DHPCT models using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DHPCTOptimizer:\n",
    "    \"\"\"DHPCTOptimizer provides hyperparameter optimization capabilities for DHPCTIndividual instances using Optuna.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, gym_name, env_props=None, n_trials=100, timeout=None, db_storage=None):\n",
    "        \"\"\"\n",
    "        Initialize a new optimizer with optimization parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - env_name: String identifier for the environment\n",
    "        - gym_name: Name of the OpenAI Gym environment to use\n",
    "        - env_props: Dictionary of environment properties\n",
    "        - n_trials: Number of trials to run\n",
    "        - timeout: Seconds to run optimization (None = no limit)\n",
    "        - db_storage: SQLite database file for storage (None = in-memory)\n",
    "        \"\"\"\n",
    "        self.env_name = env_name\n",
    "        self.gym_name = gym_name\n",
    "        self.env_props = env_props or {}\n",
    "        self.n_trials = n_trials\n",
    "        self.timeout = timeout\n",
    "        self.db_storage = db_storage\n",
    "        self.study = None\n",
    "        self.best_params = None\n",
    "        self.evaluation_episodes = 10  # Default, can be changed\n",
    "    \n",
    "    def define_objective(self, trial):\n",
    "        \"\"\"\n",
    "        Define the objective function for Optuna.\n",
    "        \n",
    "        Parameters:\n",
    "        - trial: Optuna trial object\n",
    "        \n",
    "        Returns:\n",
    "        - Fitness (average reward)\n",
    "        \"\"\"\n",
    "        # Define hyperparameters to optimize\n",
    "        n_levels = trial.suggest_int('n_levels', 1, 4)\n",
    "        \n",
    "        # Create lists to store level configurations\n",
    "        levels = []\n",
    "        activation_funcs = []\n",
    "        weight_types = []\n",
    "        \n",
    "        # Define parameters for each level\n",
    "        for i in range(n_levels):\n",
    "            # Units for this level\n",
    "            units = trial.suggest_int(f'level_{i}_units', 4, 64)\n",
    "            levels.append({'units': units})\n",
    "            \n",
    "            # Activation function for this level\n",
    "            activation_options = ['linear', 'relu', 'tanh', 'sigmoid']\n",
    "            activation = trial.suggest_categorical(f'level_{i}_activation', activation_options)\n",
    "            activation_funcs.append(activation)\n",
    "            \n",
    "            # Weight initialization for this level\n",
    "            weight_options = ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal']\n",
    "            weight_type = trial.suggest_categorical(f'level_{i}_weight', weight_options)\n",
    "            weight_types.append(weight_type)\n",
    "        \n",
    "        # Create and evaluate individual\n",
    "        individual = DHPCTIndividual(\n",
    "            env_name=self.env_name,\n",
    "            gym_name=self.gym_name,\n",
    "            env_props=self.env_props,\n",
    "            levels=levels,\n",
    "            activation_funcs=activation_funcs,\n",
    "            weight_types=weight_types\n",
    "        )\n",
    "        \n",
    "        # Evaluate the individual\n",
    "        fitness = individual.evaluate(episodes=self.evaluation_episodes)\n",
    "        return fitness\n",
    "    \n",
    "    def run_optimization(self, evaluation_episodes=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Run the hyperparameter optimization.\n",
    "        \n",
    "        Parameters:\n",
    "        - evaluation_episodes: Number of episodes for individual evaluation\n",
    "        - verbose: Whether to print progress\n",
    "        \n",
    "        Returns:\n",
    "        - Optuna study object with optimization results\n",
    "        \"\"\"\n",
    "        # Set evaluation episodes if provided\n",
    "        if evaluation_episodes is not None:\n",
    "            self.evaluation_episodes = evaluation_episodes\n",
    "        \n",
    "        # Create study storage\n",
    "        if self.db_storage is not None:\n",
    "            storage = f\"sqlite:///{self.db_storage}\"\n",
    "        else:\n",
    "            storage = None\n",
    "        \n",
    "        # Create and run the optimization study\n",
    "        sampler = optuna.samplers.TPESampler(seed=42)  # For reproducibility\n",
    "        self.study = optuna.create_study(\n",
    "            storage=storage,\n",
    "            sampler=sampler,\n",
    "            study_name=f\"dpct_{self.env_name}\",\n",
    "            direction='maximize',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        \n",
    "        self.study.optimize(\n",
    "            self.define_objective,\n",
    "            n_trials=self.n_trials,\n",
    "            timeout=self.timeout,\n",
    "            show_progress_bar=verbose\n",
    "        )\n",
    "        \n",
    "        # Store best parameters\n",
    "        self.best_params = self.study.best_params\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Best trial: {self.study.best_trial.number}\")\n",
    "            print(f\"Best value: {self.study.best_value}\")\n",
    "            print(f\"Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return self.study\n",
    "    \n",
    "    def get_best_individual(self):\n",
    "        \"\"\"\n",
    "        Create a DHPCTIndividual with the best hyperparameters.\n",
    "        \n",
    "        Returns:\n",
    "        - DHPCTIndividual with best parameters\n",
    "        \"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"No optimization results available. Run optimization first.\")\n",
    "        \n",
    "        # Extract parameters\n",
    "        n_levels = self.best_params['n_levels']\n",
    "        \n",
    "        # Create lists to store level configurations\n",
    "        levels = []\n",
    "        activation_funcs = []\n",
    "        weight_types = []\n",
    "        \n",
    "        # Create level configurations from parameters\n",
    "        for i in range(n_levels):\n",
    "            units = self.best_params[f'level_{i}_units']\n",
    "            levels.append({'units': units})\n",
    "            \n",
    "            activation = self.best_params[f'level_{i}_activation']\n",
    "            activation_funcs.append(activation)\n",
    "            \n",
    "            weight_type = self.best_params[f'level_{i}_weight']\n",
    "            weight_types.append(weight_type)\n",
    "        \n",
    "        # Create individual with best parameters\n",
    "        best_individual = DHPCTIndividual(\n",
    "            env_name=self.env_name,\n",
    "            gym_name=self.gym_name,\n",
    "            env_props=self.env_props,\n",
    "            levels=levels,\n",
    "            activation_funcs=activation_funcs,\n",
    "            weight_types=weight_types\n",
    "        )\n",
    "        \n",
    "        return best_individual\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        Visualize the optimization results.\n",
    "        \n",
    "        Returns:\n",
    "        - List of matplotlib figures\n",
    "        \"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No optimization results to visualize. Run optimization first.\")\n",
    "        \n",
    "        figures = []\n",
    "        \n",
    "        # Optimization history\n",
    "        fig = optuna.visualization.plot_optimization_history(self.study)\n",
    "        figures.append(fig)\n",
    "        \n",
    "        # Parameter importances\n",
    "        try:\n",
    "            fig = optuna.visualization.plot_param_importances(self.study)\n",
    "            figures.append(fig)\n",
    "        except:\n",
    "            # Some versions or configurations might fail on this\n",
    "            pass\n",
    "        \n",
    "        # Parallel coordinate plot for parameters\n",
    "        fig = optuna.visualization.plot_parallel_coordinate(self.study)\n",
    "        figures.append(fig)\n",
    "        \n",
    "        return figures\n",
    "    \n",
    "    def save_results(self, best_params_file=\"best_params.json\", best_individual_file=\"best_individual.json\"):\n",
    "        \"\"\"\n",
    "        Save optimization results to files.\n",
    "        \n",
    "        Parameters:\n",
    "        - best_params_file: File to save best parameters\n",
    "        - best_individual_file: File to save best individual configuration\n",
    "        \"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"No optimization results to save. Run optimization first.\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        with open(best_params_file, 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=2)\n",
    "        \n",
    "        # Save best individual\n",
    "        best_individual = self.get_best_individual()\n",
    "        best_individual.save_config(best_individual_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b89663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using DHPCTOptimizer\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = DHPCTOptimizer(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={},\n",
    "    n_trials=20,  # Small number for quick example\n",
    "    timeout=None,\n",
    "    db_storage=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (commented out as it would take time)\n",
    "# study = optimizer.run_optimization(evaluation_episodes=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best individual (commented out as it depends on optimization)\n",
    "# best_individual = optimizer.get_best_individual()\n",
    "# best_individual.compile()\n",
    "# best_individual.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4383ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from dpct.individual import DHPCTIndividual\n",
    "from dpct.evolver import DHPCTEvolver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01df9ec",
   "metadata": {},
   "source": [
    "# DHPCTOptimizer\n",
    "\n",
    "> Class for optimizing hyperparameters of the evolution process using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DHPCTOptimizer:\n",
    "    \"\"\"DHPCTOptimizer uses Optuna to perform hyperparameter optimization on the evolution process.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 evolution_params, \n",
    "                 n_trials=100, \n",
    "                 timeout=None, \n",
    "                 pruner=None, \n",
    "                 sampler=None,\n",
    "                 study_name=None,\n",
    "                 storage=None):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer with configuration parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - evolution_params: Dictionary of parameters for evolution, each with a 'fixed' flag\n",
    "                          If fixed=True, the parameter is not optimized and the provided value is used\n",
    "                          If fixed=False, the parameter is optimized within the specified range\n",
    "        - n_trials: Number of optimization trials to run\n",
    "        - timeout: Maximum time for optimization in seconds\n",
    "        - pruner: Optuna pruner instance\n",
    "        - sampler: Optuna sampler instance\n",
    "        - study_name: Name for the Optuna study\n",
    "        - storage: Optuna storage URL\n",
    "        \"\"\"\n",
    "        self.evolution_params = evolution_params\n",
    "        self.n_trials = n_trials\n",
    "        self.timeout = timeout\n",
    "        self.pruner = pruner or optuna.pruners.MedianPruner()\n",
    "        self.sampler = sampler or optuna.samplers.TPESampler()\n",
    "        self.study_name = study_name or \"DPCT_optimization\"\n",
    "        self.storage = storage\n",
    "        \n",
    "        self.study = None\n",
    "        self._objective_func = None\n",
    "        self._template_individual = None\n",
    "        self._fitness_function = None\n",
    "        self._evaluation_budget = None\n",
    "    \n",
    "    def define_objective(self, template_individual, fitness_function, evaluation_budget=None):\n",
    "        \"\"\"Define the objective function for Optuna to optimize\n",
    "        \n",
    "        Parameters:\n",
    "        - template_individual: A DHPCTIndividual to use as a template\n",
    "        - fitness_function: Function to evaluate individuals\n",
    "        - evaluation_budget: Maximum number of evaluations per trial\n",
    "        \"\"\"\n",
    "        self._template_individual = template_individual\n",
    "        self._fitness_function = fitness_function\n",
    "        self._evaluation_budget = evaluation_budget\n",
    "        \n",
    "        def objective(trial):\n",
    "            \"\"\"Objective function for Optuna\"\"\"\n",
    "            # Extract parameters for this trial\n",
    "            params = {}\n",
    "            \n",
    "            for param_name, param_config in self.evolution_params.items():\n",
    "                if param_config.get(\"fixed\", False):\n",
    "                    # Use the fixed value\n",
    "                    params[param_name] = param_config.get(\"value\")\n",
    "                else:\n",
    "                    # Optimize this parameter\n",
    "                    param_type = param_config.get(\"type\", \"float\")\n",
    "                    param_min = param_config.get(\"min\")\n",
    "                    param_max = param_config.get(\"max\")\n",
    "                    param_step = param_config.get(\"step\")\n",
    "                    param_choices = param_config.get(\"choices\")\n",
    "                    \n",
    "                    if param_type == \"float\":\n",
    "                        params[param_name] = trial.suggest_float(param_name, param_min, param_max, step=param_step)\n",
    "                    elif param_type == \"int\":\n",
    "                        params[param_name] = trial.suggest_int(param_name, param_min, param_max, step=param_step or 1)\n",
    "                    elif param_type == \"categorical\":\n",
    "                        params[param_name] = trial.suggest_categorical(param_name, param_choices)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown parameter type: {param_type}\")\n",
    "            \n",
    "            # Create an evolver with the selected parameters\n",
    "            evolver = DHPCTEvolver(\n",
    "                pop_size=params.get(\"pop_size\", 50),\n",
    "                generations=params.get(\"generations\", 100),\n",
    "                evolve_termination=params.get(\"evolve_termination\"),\n",
    "                evolve_static_termination=params.get(\"evolve_static_termination\"),\n",
    "                unchanged_generations=params.get(\"unchanged_generations\", 5),\n",
    "                run_best=params.get(\"run_best\", False),  # Don't run during optimization\n",
    "                save_arch_best=params.get(\"save_arch_best\", False),  # Don't save during optimization\n",
    "                save_arch_all=params.get(\"save_arch_all\", False)  # Don't save during optimization\n",
    "            )\n",
    "            \n",
    "            # Apply hierarchy parameters to the template individual if specified\n",
    "            individual_template = DHPCTIndividual(\n",
    "                self._template_individual.env_name,\n",
    "                self._template_individual.gym_name,\n",
    "                self._template_individual.env_props.copy(),\n",
    "                params.get(\"levels\", self._template_individual.levels[:]),\n",
    "                self._template_individual.activation_funcs.copy(),\n",
    "                self._template_individual.weight_types.copy()\n",
    "            )\n",
    "            \n",
    "            # Set up the evolution\n",
    "            evolver.setup_evolution(\n",
    "                individual_template, \n",
    "                self._fitness_function, \n",
    "                minimize=params.get(\"minimize\", True)\n",
    "            )\n",
    "            \n",
    "            # Run the evolution with limited generations for optimization\n",
    "            generations_limit = min(\n",
    "                params.get(\"generations\", 100),\n",
    "                self._evaluation_budget or params.get(\"generations\", 100)\n",
    "            )\n",
    "            evolver.generations = generations_limit\n",
    "            \n",
    "            # Run the evolution silently (no verbose output)\n",
    "            population, logbook, hof = evolver.run_evolution(verbose=False)\n",
    "            \n",
    "            # Return the best fitness (minimum or maximum depending on the problem)\n",
    "            if params.get(\"minimize\", True):\n",
    "                return min(ind.fitness.values[0] for ind in population)\n",
    "            else:\n",
    "                return max(ind.fitness.values[0] for ind in population)\n",
    "        \n",
    "        self._objective_func = objective\n",
    "        return self\n",
    "    \n",
    "    def run_optimization(self, verbose=True):\n",
    "        \"\"\"Run the hyperparameter optimization process\n",
    "        \n",
    "        Parameters:\n",
    "        - verbose: Whether to print progress information\n",
    "        \n",
    "        Returns:\n",
    "        - The Optuna study object\n",
    "        \"\"\"\n",
    "        if self._objective_func is None:\n",
    "            raise ValueError(\"Objective function not defined. Call define_objective first.\")\n",
    "        \n",
    "        # Create the study\n",
    "        self.study = optuna.create_study(\n",
    "            study_name=self.study_name,\n",
    "            storage=self.storage,\n",
    "            sampler=self.sampler,\n",
    "            pruner=self.pruner,\n",
    "            direction=\"minimize\" if self.evolution_params.get(\"minimize\", {}).get(\"value\", True) else \"maximize\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        \n",
    "        # Run the optimization\n",
    "        if verbose:\n",
    "            print(f\"Starting optimization with {self.n_trials} trials...\")\n",
    "            start_time = time.time()\n",
    "        \n",
    "        self.study.optimize(\n",
    "            self._objective_func, \n",
    "            n_trials=self.n_trials,\n",
    "            timeout=self.timeout,\n",
    "            show_progress_bar=verbose\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"Optimization completed in {total_time:.2f} seconds.\")\n",
    "            print(f\"Best trial: {self.study.best_trial.number}\")\n",
    "            print(f\"Best value: {self.study.best_value}\")\n",
    "            print(f\"Best parameters: {self.study.best_params}\")\n",
    "        \n",
    "        return self.study\n",
    "    \n",
    "    def get_best_params(self):\n",
    "        \"\"\"Get the best parameters from the optimization\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of optimized parameters\n",
    "        \"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No optimization has been run. Call run_optimization first.\")\n",
    "        \n",
    "        # Combine fixed parameters with optimized parameters\n",
    "        params = {}\n",
    "        \n",
    "        for param_name, param_config in self.evolution_params.items():\n",
    "            if param_config.get(\"fixed\", False):\n",
    "                params[param_name] = param_config.get(\"value\")\n",
    "        \n",
    "        # Add optimized parameters\n",
    "        params.update(self.study.best_params)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize the optimization results\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of plot figures\n",
    "        \"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No optimization has been run. Call run_optimization first.\")\n",
    "        \n",
    "        # Create plots\n",
    "        plots = {}\n",
    "        plots[\"optimization_history\"] = plot_optimization_history(self.study)\n",
    "        plots[\"param_importances\"] = plot_param_importances(self.study)\n",
    "        \n",
    "        # Get top 2 parameters for contour plot\n",
    "        if len(self.study.best_params) >= 2:\n",
    "            param_names = list(self.study.best_params.keys())\n",
    "            plots[\"contour\"] = plot_contour(self.study, param_names[0], param_names[1])\n",
    "        \n",
    "        return plots\n",
    "    \n",
    "    def save_results(self, path):\n",
    "        \"\"\"Save optimization results\n",
    "        \n",
    "        Parameters:\n",
    "        - path: Base path to save results to\n",
    "        \"\"\"\n",
    "        if self.study is None:\n",
    "            raise ValueError(\"No optimization has been run. Call run_optimization first.\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        with open(f\"{path}_best_params.json\", \"w\") as f:\n",
    "            json.dump(self.get_best_params(), f, indent=2)\n",
    "        \n",
    "        # Save trial history\n",
    "        trials = []\n",
    "        for trial in self.study.trials:\n",
    "            trial_dict = {\n",
    "                \"number\": trial.number,\n",
    "                \"value\": trial.value,\n",
    "                \"params\": trial.params,\n",
    "                \"state\": str(trial.state)\n",
    "            }\n",
    "            trials.append(trial_dict)\n",
    "        \n",
    "        with open(f\"{path}_trials.json\", \"w\") as f:\n",
    "            json.dump(trials, f, indent=2)\n",
    "        \n",
    "        # Save visualization plots\n",
    "        plots = self.visualize_results()\n",
    "        for name, fig in plots.items():\n",
    "            fig.write_image(f\"{path}_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40280f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evolution parameters for optimization\n",
    "evolution_params = {\n",
    "    \"pop_size\": {\"fixed\": False, \"type\": \"int\", \"min\": 10, \"max\": 100, \"step\": 10},\n",
    "    \"generations\": {\"fixed\": True, \"value\": 50},\n",
    "    \"evolve_static_termination\": {\"fixed\": True, \"value\": True},\n",
    "    \"unchanged_generations\": {\"fixed\": False, \"type\": \"int\", \"min\": 3, \"max\": 10},\n",
    "    \"minimize\": {\"fixed\": True, \"value\": False}\n",
    "}\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = DHPCTOptimizer(evolution_params, n_trials=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
