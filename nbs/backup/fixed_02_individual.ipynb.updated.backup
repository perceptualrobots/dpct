{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "default_exp individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Multiply\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46851b95",
   "metadata": {},
   "source": [
    "# DHPCTIndividual\n",
    "\n",
    "> Class for implementing Deep Perceptual Control Theory individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DHPCTIndividual:\n",
    "    \"\"\"DHPCTIndividual encapsulates an environment and a keras model representing a PCT hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, gym_name, env_props=None, levels=None, activation_funcs=None, weight_types=None):\n",
    "        \"\"\"\n",
    "        Initialize a new individual with environment and hierarchy specifications.\n",
    "        \n",
    "        Parameters:\n",
    "        - env_name: String identifier for the environment\n",
    "        - gym_name: Gym environment ID (e.g. 'CartPole-v1')\n",
    "        - env_props: Optional dictionary of environment properties\n",
    "        - levels: List of integers representing nodes per level, from perception to action\n",
    "        - activation_funcs: List of activation functions for each level\n",
    "        - weight_types: List of weight initialization types for each level\n",
    "        \"\"\"\n",
    "        self.env_name = env_name\n",
    "        self.gym_name = gym_name\n",
    "        self.env_props = env_props or {}\n",
    "        \n",
    "        # Default configuration if not provided\n",
    "        self.levels = levels or [8, 4, 2]\n",
    "        self.activation_funcs = activation_funcs or ['tanh', 'tanh', 'tanh']\n",
    "        self.weight_types = weight_types or ['normal', 'normal', 'normal']\n",
    "        \n",
    "        # Computed properties\n",
    "        self.num_levels = len(self.levels)\n",
    "        self.last_reward = 0\n",
    "        self.fitness = float('-inf')\n",
    "        self.age = 0\n",
    "        self.births = 0\n",
    "        self.params = {}\n",
    "        \n",
    "        # Create the OpenAI Gym environment\n",
    "        self.env = gym.make(self.gym_name)\n",
    "        \n",
    "        # Extract observation and action space dimensions\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        if hasattr(self.env.action_space, 'n'):\n",
    "            self.action_dim = self.env.action_space.n\n",
    "            self.discrete_actions = True\n",
    "        else:\n",
    "            self.action_dim = self.env.action_space.shape[0]\n",
    "            self.discrete_actions = False\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Keras model representing a PCT hierarchy.\n",
    "        \n",
    "        Returns:\n",
    "        - A compiled Keras model\n",
    "        \"\"\"\n",
    "        # Input layer for observations\n",
    "        inputs = Input(shape=(self.obs_dim,), name='observations')\n",
    "        \n",
    "        # Build the hierarchy\n",
    "        x = inputs\n",
    "        for i in range(self.num_levels):\n",
    "            # Perception layer at this level\n",
    "            x = Dense(self.levels[i], \n",
    "                     activation=self.activation_funcs[i],\n",
    "                     kernel_initializer=self.weight_types[i],\n",
    "                     name=f'perception_L{i}')(x)\n",
    "            \n",
    "            # Reference signals (if not the top level)\n",
    "            if i < self.num_levels - 1:\n",
    "                # Error calculation (perception - reference)\n",
    "                x = Lambda(lambda x: x, name=f'error_L{i}')(x)\n",
    "        \n",
    "        # Output layer for actions\n",
    "        if self.discrete_actions:\n",
    "            outputs = Dense(self.action_dim, activation='softmax', name='actions')(x)\n",
    "        else:\n",
    "            outputs = Dense(self.action_dim, activation='tanh', name='actions')(x)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        \"\"\"\n",
    "        Get an action from the model given an observation.\n",
    "        \n",
    "        Parameters:\n",
    "        - observation: The current environment observation\n",
    "        \n",
    "        Returns:\n",
    "        - The selected action\n",
    "        \"\"\"\n",
    "        # Reshape observation for model input\n",
    "        obs = np.array(observation).reshape(1, -1)\n",
    "        \n",
    "        # Get model prediction\n",
    "        action_probs = self.model.predict(obs, verbose=0)[0]\n",
    "        \n",
    "        if self.discrete_actions:\n",
    "            # For discrete actions, sample from probability distribution\n",
    "            action = np.random.choice(self.action_dim, p=action_probs)\n",
    "        else:\n",
    "            # For continuous actions, use the output directly\n",
    "            action = action_probs\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def run_episode(self, max_steps=1000, render=False):\n",
    "        \"\"\"\n",
    "        Run a complete episode in the environment.\n",
    "        \n",
    "        Parameters:\n",
    "        - max_steps: Maximum number of steps per episode\n",
    "        - render: Whether to render the environment\n",
    "        \n",
    "        Returns:\n",
    "        - Total episode reward\n",
    "        \"\"\"\n",
    "        observation, info = self.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        \n",
    "        while not (done or truncated) and step < max_steps:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            \n",
    "            # Get action from model\n",
    "            action = self.get_action(observation)\n",
    "            \n",
    "            # Take action in environment\n",
    "            observation, reward, done, truncated, info = self.env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        \n",
    "        self.last_reward = total_reward\n",
    "        self.fitness = max(self.fitness, total_reward)  # Update fitness if better\n",
    "        self.age += 1  # Increment age\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def mate(self, other, mutation_rate=0.1):\n",
    "        \"\"\"\n",
    "        Create offspring by combining weights with another individual.\n",
    "        \n",
    "        Parameters:\n",
    "        - other: Another DHPCTIndividual to mate with\n",
    "        - mutation_rate: Probability of mutating each weight\n",
    "        \n",
    "        Returns:\n",
    "        - A new DHPCTIndividual offspring\n",
    "        \"\"\"\n",
    "        # Create new individual with same configuration\n",
    "        offspring = DHPCTIndividual(\n",
    "            self.env_name, \n",
    "            self.gym_name,\n",
    "            self.env_props,\n",
    "            self.levels,\n",
    "            self.activation_funcs,\n",
    "            self.weight_types\n",
    "        )\n",
    "        \n",
    "        # Crossover: combine weights from parents\n",
    "        for i, layer in enumerate(offspring.model.layers):\n",
    "            if len(layer.get_weights()) > 0:  # Skip layers without weights\n",
    "                # Get parent weights\n",
    "                parent1_weights = self.model.layers[i].get_weights()\n",
    "                parent2_weights = other.model.layers[i].get_weights()\n",
    "                \n",
    "                offspring_weights = []\n",
    "                for w1, w2 in zip(parent1_weights, parent2_weights):\n",
    "                    # Crossover: randomly select weights from each parent\n",
    "                    mask = np.random.random(w1.shape) < 0.5\n",
    "                    new_weights = np.where(mask, w1, w2)\n",
    "                    \n",
    "                    # Mutation: randomly perturb weights\n",
    "                    mutation_mask = np.random.random(w1.shape) < mutation_rate\n",
    "                    mutations = np.random.normal(0, 0.1, w1.shape)\n",
    "                    new_weights = np.where(mutation_mask, new_weights + mutations, new_weights)\n",
    "                    \n",
    "                    offspring_weights.append(new_weights)\n",
    "                \n",
    "                # Set the combined weights to offspring\n",
    "                layer.set_weights(offspring_weights)\n",
    "        \n",
    "        # Update birth records\n",
    "        self.births += 1\n",
    "        other.births += 1\n",
    "        \n",
    "        return offspring\n",
    "    \n",
    "    def mutate(self, mutation_rate=0.1, mutation_scale=0.1):\n",
    "        \"\"\"\n",
    "        Mutate the weights of the individual.\n",
    "        \n",
    "        Parameters:\n",
    "        - mutation_rate: Probability of mutating each weight\n",
    "        - mutation_scale: Scale of the mutations\n",
    "        \n",
    "        Returns:\n",
    "        - Self, for method chaining\n",
    "        \"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:  # Skip layers without weights\n",
    "                weights = layer.get_weights()\n",
    "                new_weights = []\n",
    "                \n",
    "                for w in weights:\n",
    "                    # Create mutation mask and perturbations\n",
    "                    mutation_mask = np.random.random(w.shape) < mutation_rate\n",
    "                    mutations = np.random.normal(0, mutation_scale, w.shape)\n",
    "                    \n",
    "                    # Apply mutations\n",
    "                    new_w = np.where(mutation_mask, w + mutations, w)\n",
    "                    new_weights.append(new_w)\n",
    "                \n",
    "                # Update weights\n",
    "                layer.set_weights(new_weights)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def learn(self, observations, rewards, learning_rate=0.01, epochs=10):\n",
    "        \"\"\"\n",
    "        Update model weights based on observations and rewards (online learning).\n",
    "        \n",
    "        Parameters:\n",
    "        - observations: List of environment observations\n",
    "        - rewards: Corresponding rewards received\n",
    "        - learning_rate: Learning rate for weight updates\n",
    "        - epochs: Number of training epochs\n",
    "        \n",
    "        Returns:\n",
    "        - Training history\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        observations = np.array(observations)\n",
    "        rewards = np.array(rewards)\n",
    "        \n",
    "        # Get current action predictions\n",
    "        current_actions = self.model.predict(observations, verbose=0)\n",
    "        \n",
    "        # Scale rewards to use as targets\n",
    "        reward_scale = np.abs(rewards).max() if np.abs(rewards).max() > 0 else 1\n",
    "        scaled_rewards = rewards / reward_scale\n",
    "        \n",
    "        # Update action values based on rewards\n",
    "        target_actions = current_actions.copy()\n",
    "        for i in range(len(observations)):\n",
    "            if self.discrete_actions:\n",
    "                # For discrete actions, increase probability of rewarded actions\n",
    "                action_idx = np.argmax(current_actions[i])\n",
    "                target_actions[i, action_idx] += learning_rate * scaled_rewards[i]\n",
    "                # Normalize to ensure valid probability distribution\n",
    "                target_actions[i] = target_actions[i] / target_actions[i].sum()\n",
    "            else:\n",
    "                # For continuous actions, adjust action outputs based on reward\n",
    "                target_actions[i] += learning_rate * scaled_rewards[i]\n",
    "                # Clip to valid range\n",
    "                target_actions[i] = np.clip(target_actions[i], -1, 1)\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            observations, target_actions,\n",
    "            epochs=epochs,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the individual's model and configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - filepath: Path to save the model and configuration\n",
    "        \"\"\"\n",
    "        # Save model weights\n",
    "        self.model.save_weights(filepath + '_weights.h5')\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\n",
    "            'env_name': self.env_name,\n",
    "            'gym_name': self.gym_name,\n",
    "            'env_props': self.env_props,\n",
    "            'levels': self.levels,\n",
    "            'activation_funcs': self.activation_funcs,\n",
    "            'weight_types': self.weight_types,\n",
    "            'fitness': self.fitness,\n",
    "            'age': self.age,\n",
    "            'births': self.births,\n",
    "            'params': self.params\n",
    "        }\n",
    "        \n",
    "        with open(filepath + '_config.json', 'w') as f:\n",
    "            json.dump(config, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load an individual from saved model and configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - filepath: Path to the saved model and configuration\n",
    "        \n",
    "        Returns:\n",
    "        - A DHPCTIndividual instance\n",
    "        \"\"\"\n",
    "        # Load configuration\n",
    "        with open(filepath + '_config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create individual with loaded configuration\n",
    "        individual = cls(\n",
    "            config['env_name'],\n",
    "            config['gym_name'],\n",
    "            config['env_props'],\n",
    "            config['levels'],\n",
    "            config['activation_funcs'],\n",
    "            config['weight_types']\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        individual.model.load_weights(filepath + '_weights.h5')\n",
    "        \n",
    "        # Restore other attributes\n",
    "        individual.fitness = config['fitness']\n",
    "        individual.age = config['age']\n",
    "        individual.births = config['births']\n",
    "        individual.params = config['params']\n",
    "        \n",
    "        return individual"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
