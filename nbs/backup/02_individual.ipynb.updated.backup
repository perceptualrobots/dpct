{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e09d007",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: individual.html\n",
    "title: DHPCTIndividual\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa57926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c52d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5f1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nbdev import *\n",
    "# default_exp individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858eeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Multiply\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164debdf",
   "metadata": {},
   "source": [
    "# DHPCTIndividual\n",
    "\n",
    "> Class for implementing Deep Perceptual Control Theory individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3eb2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DHPCTIndividual:\n",
    "    \"\"\"DHPCTIndividual encapsulates an environment and a keras model representing a PCT hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, gym_name, env_props=None, levels=None, activation_funcs=None, weight_types=None):\n",
    "        \"\"\"\n",
    "        Initialize a new individual with environment and hierarchy specifications.\n",
    "        \n",
    "        Parameters:\n",
    "        - env_name: String identifier for the environment\n",
    "        - gym_name: Name of the OpenAI Gym environment to use\n",
    "        - env_props: Dictionary of environment properties\n",
    "        - levels: List of dictionaries specifying each level in the hierarchy\n",
    "        - activation_funcs: List of activation functions for each level\n",
    "        - weight_types: List of weight initialization methods for each level\n",
    "        \"\"\"\n",
    "        self.env_name = env_name\n",
    "        self.gym_name = gym_name\n",
    "        self.env_props = env_props or {}\n",
    "        self.levels = levels or []\n",
    "        self.activation_funcs = activation_funcs or []\n",
    "        self.weight_types = weight_types or []\n",
    "        self.env = None\n",
    "        self.model = None\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Create an individual from a configuration dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        - config: Dictionary containing individual configuration\n",
    "        \n",
    "        Returns:\n",
    "        - DHPCTIndividual instance\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            env_name=config['env_name'],\n",
    "            gym_name=config['gym_name'],\n",
    "            env_props=config.get('env_props', {}),\n",
    "            levels=config.get('levels', []),\n",
    "            activation_funcs=config.get('activation_funcs', []),\n",
    "            weight_types=config.get('weight_types', [])\n",
    "        )\n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\"\n",
    "        Build the environment and Keras model based on specifications.\n",
    "        \"\"\"\n",
    "        # Initialize the environment\n",
    "        self.env = gym.make(self.gym_name, **self.env_props)\n",
    "        \n",
    "        # Get environment dimensions\n",
    "        state_dim = self.env.observation_space.shape[0]\n",
    "        action_dim = self.env.action_space.shape[0] if hasattr(self.env.action_space, 'shape') else self.env.action_space.n\n",
    "        \n",
    "        # Build Keras model\n",
    "        inputs = Input(shape=(state_dim,), name='env_input')\n",
    "        x = inputs\n",
    "        \n",
    "        # Build hierarchy levels\n",
    "        for i, level_spec in enumerate(self.levels):\n",
    "            # Reference signal (desired perceptual signal)\n",
    "            if i == 0:\n",
    "                # First level uses external input as reference\n",
    "                reference = x\n",
    "            else:\n",
    "                # Higher levels use output from level above as reference\n",
    "                reference = Dense(level_spec['units'],\n",
    "                                name=f'reference_{i}',\n",
    "                                activation=self.activation_funcs[i] if i < len(self.activation_funcs) else 'linear')(x)\n",
    "            \n",
    "            # Perception (current perceptual signal)\n",
    "            perception = Dense(level_spec['units'],\n",
    "                             name=f'perception_{i}',\n",
    "                             activation=self.activation_funcs[i] if i < len(self.activation_funcs) else 'linear')(x)\n",
    "            \n",
    "            # Error (difference between reference and perception)\n",
    "            error = Lambda(lambda inputs: inputs[0] - inputs[1], name=f'error_{i}')([reference, perception])\n",
    "            \n",
    "            # Output (action to reduce error)\n",
    "            x = Dense(level_spec['units'],\n",
    "                    name=f'output_{i}',\n",
    "                    activation=self.activation_funcs[i] if i < len(self.activation_funcs) else 'linear')(error)\n",
    "        \n",
    "        # Final layer (outputs action to environment)\n",
    "        outputs = Dense(action_dim, name='action', activation='tanh')(x)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def config(self):\n",
    "        \"\"\"\n",
    "        Return a dictionary of individual's properties.\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary containing configuration\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'env_name': self.env_name,\n",
    "            'gym_name': self.gym_name,\n",
    "            'env_props': self.env_props,\n",
    "            'levels': self.levels,\n",
    "            'activation_funcs': self.activation_funcs,\n",
    "            'weight_types': self.weight_types\n",
    "        }\n",
    "    \n",
    "    def save_config(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the configuration to a JSON file.\n",
    "        \n",
    "        Parameters:\n",
    "        - filepath: Path to save the configuration file\n",
    "        \"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.config(), f, indent=2)\n",
    "    \n",
    "    def run(self, episodes=1, render=False, online_learning=False, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Run the individual in its environment.\n",
    "        \n",
    "        Parameters:\n",
    "        - episodes: Number of episodes to run\n",
    "        - render: Whether to render the environment\n",
    "        - online_learning: Whether to use online learning\n",
    "        - learning_rate: Learning rate for online updates\n",
    "        \n",
    "        Returns:\n",
    "        - List of episode rewards\n",
    "        \"\"\"\n",
    "        if self.env is None or self.model is None:\n",
    "            self.compile()\n",
    "        \n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # Get action\n",
    "                state_array = np.array([state])\n",
    "                action = self.model.predict(state_array, verbose=0)[0]\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Online learning\n",
    "                if online_learning:\n",
    "                    # Example online learning: Improve actions that led to rewards\n",
    "                    if reward > 0:\n",
    "                        # Create target (actual action + small push in reward direction)\n",
    "                        target = action + learning_rate * reward * np.sign(action)\n",
    "                        # Update model\n",
    "                        self.model.fit(state_array, np.array([target]), verbose=0)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def mate(self, other):\n",
    "        \"\"\"\n",
    "        Mate this individual with another to produce offspring.\n",
    "        \n",
    "        Parameters:\n",
    "        - other: Another DHPCTIndividual to mate with\n",
    "        \n",
    "        Returns:\n",
    "        - New DHPCTIndividual instance\n",
    "        \"\"\"\n",
    "        # Create new config combining properties from both parents\n",
    "        offspring_env_name = self.env_name if np.random.random() < 0.5 else other.env_name\n",
    "        offspring_gym_name = self.gym_name if np.random.random() < 0.5 else other.gym_name\n",
    "        \n",
    "        # Combine environment properties\n",
    "        offspring_env_props = {}\n",
    "        all_keys = set(list(self.env_props.keys()) + list(other.env_props.keys()))\n",
    "        for key in all_keys:\n",
    "            if key in self.env_props and key in other.env_props:\n",
    "                # If both have the property, randomly choose one\n",
    "                offspring_env_props[key] = self.env_props[key] if np.random.random() < 0.5 else other.env_props[key]\n",
    "            elif key in self.env_props:\n",
    "                # Only self has the property\n",
    "                offspring_env_props[key] = self.env_props[key]\n",
    "            else:\n",
    "                # Only other has the property\n",
    "                offspring_env_props[key] = other.env_props[key]\n",
    "        \n",
    "        # Determine hierarchy levels\n",
    "        min_levels = min(len(self.levels), len(other.levels))\n",
    "        offspring_levels = []\n",
    "        for i in range(max(len(self.levels), len(other.levels))):\n",
    "            if i < min_levels:\n",
    "                # Both parents have this level, randomly select one\n",
    "                offspring_levels.append(\n",
    "                    self.levels[i] if np.random.random() < 0.5 else other.levels[i]\n",
    "                )\n",
    "            elif i < len(self.levels):\n",
    "                # Only self has this level\n",
    "                offspring_levels.append(self.levels[i])\n",
    "            else:\n",
    "                # Only other has this level\n",
    "                offspring_levels.append(other.levels[i])\n",
    "        \n",
    "        # Combine activation functions\n",
    "        offspring_activation_funcs = []\n",
    "        for i in range(len(offspring_levels)):\n",
    "            if i < len(self.activation_funcs) and i < len(other.activation_funcs):\n",
    "                # Both parents have activation for this level\n",
    "                offspring_activation_funcs.append(\n",
    "                    self.activation_funcs[i] if np.random.random() < 0.5 else other.activation_funcs[i]\n",
    "                )\n",
    "            elif i < len(self.activation_funcs):\n",
    "                # Only self has activation for this level\n",
    "                offspring_activation_funcs.append(self.activation_funcs[i])\n",
    "            elif i < len(other.activation_funcs):\n",
    "                # Only other has activation for this level\n",
    "                offspring_activation_funcs.append(other.activation_funcs[i])\n",
    "            else:\n",
    "                # Default to linear if neither parent has activation for this level\n",
    "                offspring_activation_funcs.append('linear')\n",
    "        \n",
    "        # Combine weight types similarly\n",
    "        offspring_weight_types = []\n",
    "        for i in range(len(offspring_levels)):\n",
    "            if i < len(self.weight_types) and i < len(other.weight_types):\n",
    "                offspring_weight_types.append(\n",
    "                    self.weight_types[i] if np.random.random() < 0.5 else other.weight_types[i]\n",
    "                )\n",
    "            elif i < len(self.weight_types):\n",
    "                offspring_weight_types.append(self.weight_types[i])\n",
    "            elif i < len(other.weight_types):\n",
    "                offspring_weight_types.append(other.weight_types[i])\n",
    "            else:\n",
    "                offspring_weight_types.append('glorot_uniform')\n",
    "                \n",
    "        # Create and return new individual\n",
    "        return DHPCTIndividual(\n",
    "            env_name=offspring_env_name,\n",
    "            gym_name=offspring_gym_name,\n",
    "            env_props=offspring_env_props,\n",
    "            levels=offspring_levels,\n",
    "            activation_funcs=offspring_activation_funcs,\n",
    "            weight_types=offspring_weight_types\n",
    "        )\n",
    "    \n",
    "    def mutate(self, mutation_rate=0.1):\n",
    "        \"\"\"\n",
    "        Mutate the individual's properties.\n",
    "        \n",
    "        Parameters:\n",
    "        - mutation_rate: Probability of mutating each property\n",
    "        \n",
    "        Returns:\n",
    "        - Self (the mutated individual)\n",
    "        \"\"\"\n",
    "        # Mutate environment properties\n",
    "        for key in self.env_props:\n",
    "            if np.random.random() < mutation_rate:\n",
    "                if isinstance(self.env_props[key], (int, float)):\n",
    "                    # Add random noise to numeric properties\n",
    "                    self.env_props[key] *= np.random.normal(1.0, 0.1)\n",
    "                elif isinstance(self.env_props[key], bool):\n",
    "                    # Flip boolean properties\n",
    "                    self.env_props[key] = not self.env_props[key]\n",
    "        \n",
    "        # Mutate hierarchy levels\n",
    "        for level in self.levels:\n",
    "            if np.random.random() < mutation_rate:\n",
    "                # Mutate units in this level\n",
    "                if 'units' in level:\n",
    "                    level['units'] = max(1, int(level['units'] * np.random.normal(1.0, 0.2)))\n",
    "        \n",
    "        # Mutate activation functions\n",
    "        activation_options = ['linear', 'relu', 'tanh', 'sigmoid']\n",
    "        for i in range(len(self.activation_funcs)):\n",
    "            if np.random.random() < mutation_rate:\n",
    "                self.activation_funcs[i] = np.random.choice(activation_options)\n",
    "        \n",
    "        # Mutate weight types\n",
    "        weight_options = ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal']\n",
    "        for i in range(len(self.weight_types)):\n",
    "            if np.random.random() < mutation_rate:\n",
    "                self.weight_types[i] = np.random.choice(weight_options)\n",
    "        \n",
    "        # Reset the model so it will be recompiled with the new properties\n",
    "        self.model = None\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, episodes=5):\n",
    "        \"\"\"\n",
    "        Evaluate the individual's performance.\n",
    "        \n",
    "        Parameters:\n",
    "        - episodes: Number of episodes to run\n",
    "        \n",
    "        Returns:\n",
    "        - Average reward across episodes\n",
    "        \"\"\"\n",
    "        rewards = self.run(episodes=episodes)\n",
    "        return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d964f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m      4\u001b[0m individual \u001b[38;5;241m=\u001b[39m DHPCTIndividual(\n\u001b[0;32m      5\u001b[0m     env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     gym_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     weight_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Compile the individual\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mindividual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Display model summary\u001b[39;00m\n\u001b[0;32m     20\u001b[0m individual\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m, in \u001b[0;36mDHPCTIndividual.compile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Get environment dimensions\u001b[39;00m\n\u001b[0;32m     54\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 55\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Build Keras model\u001b[39;00m\n\u001b[0;32m     58\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(state_dim,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Example of creating and using a DHPCTIndividual\n",
    "\n",
    "# Create a simple individual for CartPole\n",
    "individual = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={},\n",
    "    levels=[\n",
    "        {'units': 10},  # Level 1\n",
    "        {'units': 5}    # Level 2\n",
    "    ],\n",
    "    activation_funcs=['relu', 'tanh'],\n",
    "    weight_types=['glorot_uniform', 'glorot_uniform']\n",
    ")\n",
    "\n",
    "# Compile the individual\n",
    "individual.compile()\n",
    "\n",
    "# Display model summary\n",
    "individual.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the individual in the environment\n",
    "rewards = individual.run(episodes=3, render=False)\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Average reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204552fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration to file\n",
    "individual.save_config(\"cartpole_individual.json\")\n",
    "\n",
    "# Load from configuration\n",
    "with open(\"cartpole_individual.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "loaded_individual = DHPCTIndividual.from_config(config)\n",
    "print(\"Successfully loaded individual from config!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ceffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "default_exp individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Multiply\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa61e8a",
   "metadata": {},
   "source": [
    "# DHPCTIndividual\n",
    "\n",
    "> Class for implementing Deep Perceptual Control Theory individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DHPCTIndividual:\n",
    "    \"\"\"DHPCTIndividual encapsulates an environment and a keras model representing a PCT hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, gym_name, env_props=None, levels=None, activation_funcs=None, weight_types=None):\n",
    "        \"\"\"\n",
    "        Initialize a new individual with environment and hierarchy specifications.\n",
    "        \n",
    "        Parameters:\n",
    "        - env_name: String identifier for the environment\n",
    "        - gym_name: The name of the OpenAI gym environment\n",
    "        - env_props: Additional environment properties\n",
    "        - levels: Array of column sizes for each level in the hierarchy\n",
    "        - activation_funcs: Dict mapping levels to activation functions\n",
    "        - weight_types: Dict specifying weight variable types\n",
    "        \"\"\"\n",
    "        self.env_name = env_name\n",
    "        self.gym_name = gym_name\n",
    "        self.env_props = env_props or {}\n",
    "        self.levels = levels or [4, 3, 2]  # Default hierarchy structure\n",
    "        self.activation_funcs = activation_funcs or {}\n",
    "        self.weight_types = weight_types or {\"all\": \"float\"}\n",
    "        self.env = None\n",
    "        self.model = None\n",
    "        self.weights = {}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config_dict):\n",
    "        \"\"\"Create an individual from a configuration dictionary\"\"\"\n",
    "        # Extract environment config\n",
    "        env_config = config_dict.get(\"env\", {})\n",
    "        env_name = env_config.get(\"name\")\n",
    "        gym_name = env_config.get(\"gym_name\")\n",
    "        env_props = env_config.get(\"properties\", {})\n",
    "        \n",
    "        # Extract hierarchy config\n",
    "        hierarchy_config = config_dict.get(\"hierarchy\", {})\n",
    "        levels = hierarchy_config.get(\"levels\")\n",
    "        activation_funcs = hierarchy_config.get(\"activation_funcs\", {})\n",
    "        weight_types = hierarchy_config.get(\"weight_types\", {\"all\": \"float\"})\n",
    "        \n",
    "        # Create individual\n",
    "        individual = cls(env_name, gym_name, env_props, levels, activation_funcs, weight_types)\n",
    "        \n",
    "        # Load weights if present\n",
    "        if \"weights\" in config_dict:\n",
    "            individual.weights = config_dict[\"weights\"]\n",
    "        \n",
    "        return individual\n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\"Build the environment and Keras model\"\"\"\n",
    "        # Create the environment\n",
    "        self.env = gym.make(self.gym_name, **self.env_props)\n",
    "        \n",
    "        # Build the model\n",
    "        self._build_model()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the keras model for the PCT hierarchy\"\"\"\n",
    "        # Create inputs\n",
    "        perception_inputs = Input(shape=(self.env.observation_space.shape[0],), name=\"perception_inputs\")\n",
    "        reference_inputs = Input(shape=(self.levels[-1],), name=\"reference_inputs\")\n",
    "        \n",
    "        # Create level 0 (lowest level)\n",
    "        perception_layer_0 = Dense(\n",
    "            self.levels[0],\n",
    "            activation=self.activation_funcs.get(0, \"linear\"),\n",
    "            name=\"perception_0\"\n",
    "        )(perception_inputs)\n",
    "        \n",
    "        perception_layers = [perception_layer_0]\n",
    "        output_layers = []\n",
    "        comparator_layers = []\n",
    "        reference_layers = []\n",
    "        \n",
    "        # Build the hierarchy\n",
    "        for i in range(1, len(self.levels)):\n",
    "            # Create perception layer for this level\n",
    "            perception_layer = Dense(\n",
    "                self.levels[i],\n",
    "                activation=self.activation_funcs.get(i, \"linear\"),\n",
    "                name=f\"perception_{i}\"\n",
    "            )(perception_layers[i-1])\n",
    "            perception_layers.append(perception_layer)\n",
    "        \n",
    "        # Create reference layers (top-down)\n",
    "        # Top level reference is the input reference\n",
    "        reference_layers.append(reference_inputs)\n",
    "        \n",
    "        # Create comparator for top level\n",
    "        top_level = len(self.levels) - 1\n",
    "        comparator_top = Lambda(\n",
    "            lambda x: x[0] - x[1],\n",
    "            name=f\"comparator_{top_level}\"\n",
    "        )([reference_layers[0], perception_layers[top_level]])\n",
    "        comparator_layers.append(comparator_top)\n",
    "        \n",
    "        # Create output layer for top level\n",
    "        output_top = Dense(\n",
    "            self.levels[top_level-1],\n",
    "            activation=self.activation_funcs.get(top_level, \"linear\"),\n",
    "            name=f\"output_{top_level}\"\n",
    "        )(comparator_top)\n",
    "        output_layers.append(output_top)\n",
    "        \n",
    "        # Build remaining levels (top-down)\n",
    "        for i in range(top_level-1, -1, -1):\n",
    "            # Create reference layer from output of level above\n",
    "            reference_layer = Dense(\n",
    "                self.levels[i],\n",
    "                activation=self.activation_funcs.get(i, \"linear\"),\n",
    "                name=f\"reference_{i}\"\n",
    "            )(output_layers[0] if i == top_level-1 else output_layers[-1])\n",
    "            reference_layers.insert(0, reference_layer)\n",
    "            \n",
    "            # Create comparator\n",
    "            comparator = Lambda(\n",
    "                lambda x: x[0] - x[1],\n",
    "                name=f\"comparator_{i}\"\n",
    "            )([reference_layers[0], perception_layers[i]])\n",
    "            comparator_layers.insert(0, comparator)\n",
    "            \n",
    "            # Create output layer (if not the lowest level)\n",
    "            if i > 0:\n",
    "                output_layer = Dense(\n",
    "                    self.levels[i-1],\n",
    "                    activation=self.activation_funcs.get(i, \"linear\"),\n",
    "                    name=f\"output_{i}\"\n",
    "                )(comparator)\n",
    "                output_layers.insert(0, output_layer)\n",
    "        \n",
    "        # Level 0 output is the action\n",
    "        action_output = Dense(\n",
    "            self.env.action_space.n if hasattr(self.env.action_space, 'n') else self.env.action_space.shape[0],\n",
    "            activation=\"tanh\" if not hasattr(self.env.action_space, 'n') else \"softmax\",\n",
    "            name=\"action\"\n",
    "        )(comparator_layers[0])\n",
    "        \n",
    "        # Combine all errors for training\n",
    "        errors_output = Lambda(\n",
    "            lambda x: tf.concat(x, axis=1),\n",
    "            name=\"errors\"\n",
    "        )(comparator_layers)\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = Model(\n",
    "            inputs=[perception_inputs, reference_inputs],\n",
    "            outputs=[action_output, errors_output]\n",
    "        )\n",
    "        \n",
    "        # Set weights if they exist\n",
    "        if self.weights:\n",
    "            self._set_weights()\n",
    "        \n",
    "        # Compile the model for training\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss={\"action\": \"mse\", \"errors\": \"mse\"}\n",
    "        )\n",
    "    \n",
    "    def _set_weights(self):\n",
    "        \"\"\"Set the weights of the model from the stored weights dictionary\"\"\"\n",
    "        # Implementation to set model weights from self.weights\n",
    "        pass\n",
    "    \n",
    "    def config(self):\n",
    "        \"\"\"Return a dictionary of the individual's properties\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Complete configuration dictionary with environment, hierarchy, and weight data\n",
    "        \"\"\"\n",
    "        # Extract current weights if model exists\n",
    "        if self.model is not None:\n",
    "            self._extract_weights()\n",
    "            \n",
    "        config = {\n",
    "            \"env\": {\n",
    "                \"name\": self.env_name,\n",
    "                \"gym_name\": self.gym_name,\n",
    "                \"properties\": self.env_props\n",
    "            },\n",
    "            \"hierarchy\": {\n",
    "                \"levels\": self.levels,\n",
    "                \"activation_funcs\": self.activation_funcs,\n",
    "                \"weight_types\": self.weight_types\n",
    "            },\n",
    "            \"weights\": self.weights\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _extract_weights(self):\n",
    "        \"\"\"Extract weights from the model into a dictionary\"\"\"\n",
    "        # Implementation to extract model weights to self.weights\n",
    "        self.weights = {}\n",
    "        for layer in self.model.layers:\n",
    "            if layer.weights:\n",
    "                layer_weights = {}\n",
    "                for w in layer.weights:\n",
    "                    # Convert to Python list for JSON serialization\n",
    "                    weights_array = w.numpy().tolist()\n",
    "                    layer_weights[w.name] = weights_array\n",
    "                self.weights[layer.name] = layer_weights\n",
    "    \n",
    "    def save_config(self, filepath):\n",
    "        \"\"\"Save the individual's configuration to a JSON file\n",
    "        \n",
    "        Parameters:\n",
    "        - filepath: Path where the configuration will be saved\n",
    "        \n",
    "        Returns:\n",
    "        - bool: True if save was successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config()\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving configuration: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run(self, steps, train=False, early_termination=False):\n",
    "        \"\"\"Run the individual in its environment\n",
    "        \n",
    "        Parameters:\n",
    "        - steps: Number of timesteps to run\n",
    "        - train: Whether to enable online learning during execution\n",
    "        - early_termination: Whether to terminate early based on environment signals\n",
    "        \"\"\"\n",
    "        if self.env is None or self.model is None:\n",
    "            self.compile()\n",
    "        \n",
    "        observation, _ = self.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Online learning setup if enabled\n",
    "        if train:\n",
    "            # Set up training targets (zeros for all comparator outputs)\n",
    "            error_shape = sum(self.levels)\n",
    "            zero_targets = np.zeros((1, error_shape))\n",
    "        \n",
    "        for step in range(steps):\n",
    "            # Set reference inputs (could be expanded for more complex reference setting)\n",
    "            reference = np.zeros((1, self.levels[-1]))\n",
    "            \n",
    "            # Get action from model\n",
    "            observation_reshaped = np.array(observation).reshape(1, -1)\n",
    "            action_prob, errors = self.model.predict([observation_reshaped, reference], verbose=0)\n",
    "            \n",
    "            # Convert to discrete action if needed\n",
    "            if hasattr(self.env.action_space, 'n'):\n",
    "                action = np.argmax(action_prob[0])\n",
    "            else:\n",
    "                action = action_prob[0]\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Online learning if enabled\n",
    "            if train and step % 10 == 0:  # Train every 10 steps\n",
    "                self.model.train_on_batch(\n",
    "                    x=[observation_reshaped, reference],\n",
    "                    y={\"errors\": zero_targets}\n",
    "                )\n",
    "            \n",
    "            if done and early_termination:\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def mate(self, other):\n",
    "        \"\"\"Create two new individuals by crossing this one with another\"\"\"\n",
    "        # Simple implementation - can be enhanced with DEAP\n",
    "        child1 = DHPCTIndividual(\n",
    "            self.env_name, \n",
    "            self.gym_name,\n",
    "            self.env_props.copy(),\n",
    "            self.levels[:],\n",
    "            self.activation_funcs.copy(),\n",
    "            self.weight_types.copy()\n",
    "        )\n",
    "        \n",
    "        child2 = DHPCTIndividual(\n",
    "            other.env_name, \n",
    "            other.gym_name,\n",
    "            other.env_props.copy(),\n",
    "            other.levels[:],\n",
    "            other.activation_funcs.copy(),\n",
    "            other.weight_types.copy()\n",
    "        )\n",
    "        \n",
    "        # Extract and cross weights if models exist\n",
    "        if self.model is not None and other.model is not None:\n",
    "            self._extract_weights()\n",
    "            other._extract_weights()\n",
    "            # Cross weights logic here\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def mutate(self, struct_prob=0.1, weight_prob=0.1):\n",
    "        \"\"\"Mutate this individual's structure and/or weights\"\"\"\n",
    "        # Structure mutation\n",
    "        if np.random.random() < struct_prob:\n",
    "            # Randomly change a level size\n",
    "            level_to_change = np.random.randint(0, len(self.levels))\n",
    "            self.levels[level_to_change] = max(1, self.levels[level_to_change] + np.random.randint(-1, 2))\n",
    "            \n",
    "            # Need to rebuild model after structure change\n",
    "            self.model = None\n",
    "            self.weights = {}\n",
    "        \n",
    "        # Weight mutation\n",
    "        if np.random.random() < weight_prob and self.model is not None:\n",
    "            self._extract_weights()\n",
    "            # Weight mutation logic here\n",
    "            # Need to rebuild model to apply weight changes\n",
    "            self.model = None\n",
    "            self.compile()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, nevals=1):\n",
    "        \"\"\"Evaluate this individual's fitness\"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(nevals):\n",
    "            reward = self.run(steps=1000, early_termination=True)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # Return average reward\n",
    "        return sum(rewards) / len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22eafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creating an individual\n",
    "individual = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\", \n",
    "    env_props={\"render_mode\": \"rgb_array\"},\n",
    "    levels=[4, 3, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test config and save_config methods\n",
    "config = individual.config()\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cff524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
