{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11867eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from dpct.individual import DHPCTIndividual\n",
    "from dpct.evolver import DHPCTEvolver\n",
    "from dpct.optimizer import DHPCTOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e858e",
   "metadata": {},
   "source": [
    "# DPCT Usage Examples\n",
    "\n",
    "> Complete examples of using the DPCT library for deep perceptual control theory applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7642a98",
   "metadata": {},
   "source": [
    "## 1. Basic Individual Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94625609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple individual for CartPole environment\n",
    "individual = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={},\n",
    "    levels=[\n",
    "        {'units': 16},  # Level 1\n",
    "        {'units': 8}    # Level 2\n",
    "    ],\n",
    "    activation_funcs=['relu', 'tanh'],\n",
    "    weight_types=['glorot_uniform', 'glorot_uniform']\n",
    ")\n",
    "\n",
    "# Compile the individual\n",
    "individual.compile()\n",
    "\n",
    "# Display model summary\n",
    "individual.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the individual in the environment (with rendering disabled for notebook)\n",
    "rewards = individual.run(episodes=5, render=False)\n",
    "print(f\"Rewards per episode: {rewards}\")\n",
    "print(f\"Average reward: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with online learning enabled\n",
    "rewards_with_learning = individual.run(episodes=5, render=False, online_learning=True, learning_rate=0.01)\n",
    "print(f\"Rewards with online learning: {rewards_with_learning}\")\n",
    "print(f\"Average reward with learning: {np.mean(rewards_with_learning)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration to file\n",
    "individual.save_config(\"cartpole_individual_config.json\")\n",
    "\n",
    "# Load from configuration\n",
    "with open(\"cartpole_individual_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "loaded_individual = DHPCTIndividual.from_config(config)\n",
    "print(\"Successfully loaded individual from config!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f24f42c",
   "metadata": {},
   "source": [
    "## 2. Evolution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template individual for evolution\n",
    "template = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={},\n",
    "    levels=[\n",
    "        {'units': 16},\n",
    "        {'units': 8}\n",
    "    ],\n",
    "    activation_funcs=['relu', 'tanh'],\n",
    "    weight_types=['glorot_uniform', 'glorot_uniform']\n",
    ")\n",
    "\n",
    "# Initialize the evolver with small population and generations for demonstration\n",
    "evolver = DHPCTEvolver(\n",
    "    population_size=5,   # Small size for demo\n",
    "    gens=3,              # Few generations for demo\n",
    "    cx_prob=0.7,\n",
    "    mut_prob=0.2,\n",
    "    elite_size=1,\n",
    "    env_template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evolution\n",
    "evolver.setup_evolution(evaluation_episodes=2, mutation_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evolution (uncomment to run, but it will take time)\n",
    "# pop, log = evolver.run_evolution(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evolution results (commented out as it depends on evolution running)\n",
    "'''\n",
    "evolver.save_results(\n",
    "    population_file=\"cartpole_population.json\",\n",
    "    logbook_file=\"cartpole_evolution_log.json\",\n",
    "    best_file=\"cartpole_best_evolved.json\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution progress (commented out as it depends on evolution running)\n",
    "'''\n",
    "fig = evolver.plot_evolution()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21667c",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer with small number of trials for demonstration\n",
    "optimizer = DHPCTOptimizer(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={},\n",
    "    n_trials=5,    # Small number for demo\n",
    "    timeout=None,\n",
    "    db_storage=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e586db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (commented out as it would take time)\n",
    "'''\n",
    "study = optimizer.run_optimization(evaluation_episodes=2, verbose=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c48316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best individual and evaluate it (commented out as it depends on optimization)\n",
    "'''\n",
    "best_individual = optimizer.get_best_individual()\n",
    "best_individual.compile()\n",
    "print(\"Best Individual Model Summary:\")\n",
    "best_individual.model.summary()\n",
    "\n",
    "# Evaluate best individual\n",
    "rewards = best_individual.run(episodes=5, render=False)\n",
    "print(f\"Best individual rewards: {rewards}\")\n",
    "print(f\"Average reward: {np.mean(rewards)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08633cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimization results (commented out as it depends on optimization running)\n",
    "'''\n",
    "optimizer.save_results(\n",
    "    best_params_file=\"cartpole_best_params.json\",\n",
    "    best_individual_file=\"cartpole_best_optimized.json\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf718f",
   "metadata": {},
   "source": [
    "## 4. Combined Example: Optimize, Evolve, and Use Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a conceptual example of how all components can work together\n",
    "# Each step can be time-consuming, so the code is provided as a reference\n",
    "\n",
    "'''\n",
    "# Step 1: Optimize hyperparameters\n",
    "optimizer = DHPCTOptimizer(\"CartPole\", \"CartPole-v1\", n_trials=20)\n",
    "optimizer.run_optimization(evaluation_episodes=3)\n",
    "template = optimizer.get_best_individual()\n",
    "\n",
    "# Step 2: Evolve population based on optimized template\n",
    "evolver = DHPCTEvolver(population_size=20, gens=20, env_template=template)\n",
    "evolver.run_evolution()\n",
    "best_individual = evolver.best_individual\n",
    "\n",
    "# Step 3: Fine-tune with online learning\n",
    "best_individual.compile()\n",
    "rewards = best_individual.run(episodes=20, online_learning=True, learning_rate=0.01)\n",
    "\n",
    "# Display final results\n",
    "print(f\"Final performance: {np.mean(rewards)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939729b1",
   "metadata": {},
   "source": [
    "## 5. Using with Different Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a different OpenAI Gym environment: MountainCar\n",
    "# Note that this environment has a different observation and action space\n",
    "\n",
    "'''\n",
    "# Create individual for MountainCar\n",
    "mountain_car = DHPCTIndividual(\n",
    "    env_name=\"MountainCar\",\n",
    "    gym_name=\"MountainCar-v0\",\n",
    "    env_props={},\n",
    "    levels=[\n",
    "        {'units': 24},\n",
    "        {'units': 12},\n",
    "        {'units': 6}\n",
    "    ],\n",
    "    activation_funcs=['relu', 'tanh', 'tanh'],\n",
    "    weight_types=['glorot_uniform', 'glorot_uniform', 'glorot_uniform']\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "mountain_car.compile()\n",
    "mountain_car.model.summary()\n",
    "rewards = mountain_car.run(episodes=3, render=False)\n",
    "print(f\"MountainCar rewards: {rewards}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3274e",
   "metadata": {},
   "source": [
    "# DPCT Usage Examples\n",
    "\n",
    "> Examples of using the Deep Perceptual Control Theory library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90801fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DPCT components\n",
    "from dpct.individual import DHPCTIndividual\n",
    "from dpct.evolver import DHPCTEvolver\n",
    "from dpct.optimizer import DHPCTOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcaabd5",
   "metadata": {},
   "source": [
    "## 1. Creating and Running an Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an individual for the CartPole environment\n",
    "individual = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={\"render_mode\": \"rgb_array\"},\n",
    "    levels=[4, 3, 2],  # 4 inputs in lowest level, 3 in middle, 2 in highest\n",
    "    activation_funcs={0: \"linear\", 1: \"relu\", 2: \"tanh\"},\n",
    "    weight_types={\"all\": \"float\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the individual (creates environment and builds model)\n",
    "individual.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65138a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the individual for 100 steps\n",
    "reward = individual.run(steps=100, early_termination=True)\n",
    "print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with online learning enabled\n",
    "reward = individual.run(steps=100, train=True, early_termination=True)\n",
    "print(f\"Reward with training: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8aaaa",
   "metadata": {},
   "source": [
    "## 2. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current configuration\n",
    "config = individual.config()\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration to file\n",
    "individual.save_config(\"individual_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and create a new individual\n",
    "with open(\"individual_config.json\", \"r\") as f:\n",
    "    loaded_config = json.load(f)\n",
    "\n",
    "new_individual = DHPCTIndividual.from_config(loaded_config)\n",
    "new_individual.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa31e0",
   "metadata": {},
   "source": [
    "## 3. Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fitness function\n",
    "def fitness_function(individual):\n",
    "    # For CartPole, we want to maximize the reward, so return negative reward\n",
    "    # (since DEAP minimizes by default)\n",
    "    return -individual.evaluate(nevals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evolver\n",
    "evolver = DHPCTEvolver(\n",
    "    pop_size=10,  # Small population for example\n",
    "    generations=5,  # Few generations for example\n",
    "    evolve_static_termination=True,\n",
    "    unchanged_generations=3,\n",
    "    run_best=True,\n",
    "    save_arch_best=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c98df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evolution\n",
    "evolver.setup_evolution(\n",
    "    template_individual=individual,\n",
    "    fitness_function=fitness_function,\n",
    "    minimize=True  # We're minimizing negative reward\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792257a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evolution (commented out for notebook example as it takes time)\n",
    "# population, logbook, hof = evolver.run_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae58b33",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f16962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evolution parameters for optimization\n",
    "evolution_params = {\n",
    "    \"pop_size\": {\"fixed\": False, \"type\": \"int\", \"min\": 5, \"max\": 20, \"step\": 5},\n",
    "    \"generations\": {\"fixed\": True, \"value\": 5},\n",
    "    \"evolve_static_termination\": {\"fixed\": True, \"value\": True},\n",
    "    \"unchanged_generations\": {\"fixed\": False, \"type\": \"int\", \"min\": 2, \"max\": 4},\n",
    "    \"minimize\": {\"fixed\": True, \"value\": True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "optimizer = DHPCTOptimizer(\n",
    "    evolution_params=evolution_params,\n",
    "    n_trials=3,  # Few trials for example\n",
    "    pruner=optuna.pruners.MedianPruner()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f509bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective\n",
    "optimizer.define_objective(\n",
    "    template_individual=individual,\n",
    "    fitness_function=fitness_function,\n",
    "    evaluation_budget=5  # Limit evaluation budget for example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (commented out for notebook example as it takes time)\n",
    "# study = optimizer.run_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters (example output)\n",
    "best_params = {\n",
    "    \"pop_size\": 15,\n",
    "    \"generations\": 5,\n",
    "    \"evolve_static_termination\": True,\n",
    "    \"unchanged_generations\": 3,\n",
    "    \"minimize\": True\n",
    "}\n",
    "print(\"Best parameters:\")\n",
    "print(json.dumps(best_params, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a3fcb",
   "metadata": {},
   "source": [
    "## 5. Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile an individual for online learning\n",
    "online_learner = DHPCTIndividual(\n",
    "    env_name=\"CartPole\",\n",
    "    gym_name=\"CartPole-v1\",\n",
    "    env_props={\"render_mode\": \"rgb_array\"},\n",
    "    levels=[4, 3, 2]\n",
    ")\n",
    "online_learner.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b53ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first without training to establish baseline\n",
    "print(\"Running without training...\")\n",
    "rewards_without_training = []\n",
    "for _ in range(5):\n",
    "    reward = online_learner.run(steps=100, train=False, early_termination=True)\n",
    "    rewards_without_training.append(reward)\n",
    "    print(f\"Reward: {reward}\")\n",
    "\n",
    "print(f\"Average reward without training: {sum(rewards_without_training) / len(rewards_without_training)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with online learning\n",
    "print(\"Running with training...\")\n",
    "rewards_with_training = []\n",
    "for i in range(10):\n",
    "    reward = online_learner.run(steps=100, train=True, early_termination=True)\n",
    "    rewards_with_training.append(reward)\n",
    "    print(f\"Episode {i+1} reward: {reward}\")\n",
    "\n",
    "print(f\"Average reward with training: {sum(rewards_with_training) / len(rewards_with_training)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_with_training, 'b-', label='With Online Learning')\n",
    "plt.axhline(y=sum(rewards_without_training) / len(rewards_without_training), \n",
    "           color='r', linestyle='-', label='Without Training Baseline')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Online Learning Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
